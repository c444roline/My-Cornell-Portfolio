# -*- coding: utf-8 -*-
"""DefineAndSolveMLProblem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ppdjCads3rIgpurlYMhW9hv03RX5WmZw

# Lab 8: Define and Solve an ML Problem of Your Choosing
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns

"""In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan.

You will complete the following tasks:

1. Build Your DataFrame
2. Define Your ML Problem
3. Perform exploratory data analysis to understand your data.
4. Define Your Project Plan
5. Implement Your Project Plan:
    * Prepare your data for your model.
    * Fit your model to the training data and evaluate your model.
    * Improve your model's performance.

## Part 1: Build Your DataFrame

You will have the option to choose one of four data sets that you have worked with in this program:

* The "census" data set that contains Census information from 1994: `censusData.csv`
* Airbnb NYC "listings" data set: `airbnbListingsData.csv`
* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`
* Book Review data set: `bookReviewsData.csv`

Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models.

#### Load a Data Set and Save it as a Pandas DataFrame

The code cell below contains filenames (path + filename) for each of the four data sets available to you.

<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`.

You can load each file as a new DataFrame to inspect the data before choosing your data set.
"""

# File names of the four data sets
adultDataSet_filename = os.path.join(os.getcwd(), "data", "censusData.csv")
airbnbDataSet_filename = os.path.join(os.getcwd(), "data", "airbnbListingsData.csv")
WHRDataSet_filename = os.path.join(os.getcwd(), "data", "WHR2018Chapter2OnlineData.csv")
bookReviewDataSet_filename = os.path.join(os.getcwd(), "data", "bookReviewsData.csv")


df = pd.read_csv(WHRDataSet_filename)# YOUR CODE HERE

df.head()

"""## Part 2: Define Your ML Problem

Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:

1. List the data set you have chosen.
2. What will you be predicting? What is the label?
3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?
4. What are your features? (note: this list may change after your explore your data)
5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?

1 - I've chosen the World Happiness Report dataset.

2- I'll be predicting the average life expectancy of each company based on the other aspects of it. The label is 'Healthy life expectancy at birth', also known as 'Life'.

3 - This is a supervised learning problem using regression.

4 - At this point, the features will be things other than Life/healthy life expectancy at birth, which are 'country', 'year', 'Happiness', 'LogGDP', 'Support', 'Life', 'Freedom', 'Generosity', 'Corruption', 'Positive', 'Negative', 'Confidence in national government', 'Democratic Quality','Delivery Quality', 'Standard deviation of ladder by country-year','Standard deviation/Mean of ladder by country-year','GINI index (World Bank estimate)','GINI index (World Bank estimate), average 2000-15','gini of household income reported in Gallup, by wp5-year'

5 - This is an important problem because it can help policymakers/government/etc learn about the relationship between life expectancy in their country and attribute high/low numbers with perhaps corruption, or how generous they are, items like that, and then work on ways to improve or diminish factors that are causing low or high life expectancy.

## Part 3: Understand Your Data

The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:

1. What data preparation techniques would you like to use? These data preparation techniques may include:

    * addressing missingness, such as replacing missing values with means
    * finding and replacing outliers
    * renaming features and labels
    * finding and replacing outliers
    * performing feature engineering techniques such as one-hot encoding on categorical features
    * selecting appropriate features and removing irrelevant features
    * performing specific data cleaning and preprocessing techniques for an NLP problem
    * addressing class imbalance in your data sample to promote fair AI
    

2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?
    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?


3. How will you evaluate and improve the model's performance?
    * Are there specific evaluation metrics and methods that are appropriate for your model?
    

Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.

<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.

<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu.
"""

df.describe()

df.isnull().sum().sort_values(ascending=False)

df=df.drop(columns=['Confidence in national government', 'Democratic Quality', 'Delivery Quality', 'GINI index (World Bank estimate)', 'GINI index (World Bank estimate), average 2000-15', 'gini of household income reported in Gallup, by wp5-year', 'Standard deviation of ladder by country-year',
 'Standard deviation/Mean of ladder by country-year'],axis=1)
df

columnslist=df.columns.tolist()
for col in columnslist:
    if df[col].isnull().sum() > 0:
        sns.boxplot(data=df[col])
        plt.title('Box Plot of'+str(col))
        plt.show()

"""## Part 4: Define Your Project Plan

Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:

* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data?
* Explain different data preparation techniques that you will use to prepare your data for modeling.
* What is your model (or models)?
* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data.

1) The features are mostly the same, but I'm removing columns with lots (more than 100) of null values such as Confidence in national government, Democratic Quality, Delivery Quality, GINI index (World Bank estimate), GINI index (World Bank estimate), average 2000-15, gini of household income reported in Gallup, by wp5-year. I also want to get rid of Standard deviation of ladder by country-year, Standard deviation/Mean of ladder by country-year, because I'm not exactly sure what that means.
2) Looking for any missing values and remove them or replacing the missing values with a mean/median depending on the context. I won't be doing mode since it's mostly numerical and less categorical data. Specifically, I will be removing columns with lots of null values. For the imputation part, I'm going to look at boxplots of the data and if there's skew, I'll use median in imputation, but if it's mostly symmetrical with few outliers, I'll use mean. I'm also going to use one-hot encoding because there's a few different rows for the same countries and I wouldn't want the model to develop some kind of association by the order in which countries appear in the dataset and rank them.
3) My model is going to be Linear Regression and I also want to capturer any non linear relationships with random forest regression and also try gradient boosting regression.
4) I'm going to do a train/test split for modeling. Then I will do linear regression and random forest. After that I will get the residuals of the preedicted values from those models and the test values. Then I will plot the residuals ot identify any patterns or outliers. Then, I will compare the linear regression and random forest and choose which one has better performance and is most interpretable. Then from there, if there's any over or underfiting, I will consider regularization or changing the number of features. Then I will train the final model again and validate it.

## Part 5: Implement Your Project Plan

<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan.
"""

# YOUR CODE HERE
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score

"""<b>Task:</b> Use the rest of this notebook to carry out your project plan.

You will:

1. Prepare your data for your model.
2. Fit your model to the training data and evaluate your model.
3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.

Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit.
"""

cols_symmetric=['Log GDP per capita','Positive affect']
cols_skewed=['Social support', 'Healthy life expectancy at birth','Freedom to make life choices','Generosity','Perceptions of corruption','Negative affect']

for col in cols_skewed:
    df[col].fillna(df[col].median(), inplace=True)

for col in cols_symmetric:
    df[col].fillna(df[col].mean(), inplace=True)

# YOUR CODE HERE
new_names_dict = {'Life Ladder': 'Happiness',
'Log GDP per capita': 'LogGDP',
'Social support': 'Support',
'Healthy life expectancy at birth': 'Life',
'Freedom to make life choices': 'Freedom',
'Perceptions of corruption': 'Corruption',
'Positive affect': 'Positive',
'Negative affect': 'Negative'}

df.rename(columns = new_names_dict, inplace = True)

encoder = OneHotEncoder()
country_encoded = encoder.fit_transform(df[['country']]).toarray()
country_encoded_df = pd.DataFrame(country_encoded, columns=encoder.get_feature_names_out(['country']))
df_encoded = df.join(country_encoded_df).drop('country', axis=1)

X = df_encoded.drop('Life', axis=1)
y = df_encoded['Life']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lin_reg = LinearRegression()
random_forest = RandomForestRegressor(random_state=1234)

lin_reg.fit(X_train, y_train)
random_forest.fit(X_train, y_train)

param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}

grid_search = GridSearchCV(estimator=Lasso(), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", -grid_search.best_score_)

def evaluate_model(y_true, y_pred):
    """
    y_true: numerical value indicating the true y value
    y_pred: numerical value predicted to be y by the respective model
    calculates the error between the predicted y and the true y values
    """
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = root_mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, rmse, r2

y_lr = lin_reg.predict(X_test)
y_rf =random_forest.predict(X_test)

metrics_lr1 = evaluate_model(y_train[:len(y_lr)], y_lr)
metrics_lr2 = evaluate_model(y_test, y_lr)
print("training linear regression",metrics_lr1)
print("testing linear regression",metrics_lr2)
metrics_rf1 = evaluate_model(y_train[:len(y_rf)], y_rf)
metrics_rf = evaluate_model(y_test, y_rf)
print("training random forest",metrics_rf1)
print("testing random forest",metrics_rf)

"""It does not seem that there is under or overfitting as the errors are low and r^2 is high for the test data's metrics, but it seems linear regressions looks much better"""

def plot_residuals(y_true, y_pred, model_name):
    """
    y_true: numerical value indicating the true y value
    y_pred: numerical value predicted to be y by the respective model
    model_name: the model
    """
    residuals = y_true - y_pred
    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.hlines(0, min(y_pred), max(y_pred), colors='r')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title(f'Residual Plot for {model_name}')
    plt.show()

plot_residuals(y_test, y_lr, 'Linear Regression')
plot_residuals(y_test, y_rf, 'Random Forest')

"""The linear regression might be better because it has much less outliers and less extreme residuals."""

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_lr, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r-', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Predicted vs Actual Values of Linear regression model')
plt.show()

"""It looks like the linear regression model does a fairly good job at predicting values."""

X_final_train = pd.concat([X_train, X_test])
y_final_train = pd.concat([y_train, y_test])

#retraining te final model
final_model = Lasso(alpha=grid_search.best_params_['alpha'])  #replacing with the best model and parameters using lasso regression and grid search
final_model.fit(X_final_train, y_final_train)

y_final =final_model.predict(X_test)

metrics_final = evaluate_model(y_test, y_final)
print(metrics_final)

plot_residuals(y_test, y_final, 'Final Linear Regression')

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_final, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r-', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Predicted vs Actual Values of Linear regression model')
plt.show()

